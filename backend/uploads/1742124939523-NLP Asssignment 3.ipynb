{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "97efc046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import random\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "# Path to the dataset folder\n",
    "DATASET_PATH = \"dataset/\"\n",
    "\n",
    "# Load all diary entries from files\n",
    "def load_corpus():\n",
    "    corpus = []\n",
    "    for filename in os.listdir(DATASET_PATH):\n",
    "        if filename.endswith(\".txt\"):  # Assuming files are .txt\n",
    "            with open(os.path.join(DATASET_PATH, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                corpus.append(file.read().lower())  # Convert to lowercase for consistency\n",
    "    return \" \".join(corpus)  # Merge all diaries into a single corpus\n",
    "\n",
    "# Load dataset\n",
    "text_corpus = load_corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d0dda3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tokens: ['aj', 'subha', 'main', '8:15', 'pa', 'utha', 'aur', 'phir', 'main', 'washroom', 'fresh', 'hona', 'chala', 'gya', 'fresh', 'hona', 'ka', 'baad', 'main', 'kapra']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def tokenize_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\":\", \"\")))  # Remove punctuation but keep ':'\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenize words\n",
    "    \n",
    "    # Remove standalone numbers (1, 2, etc.) but keep time formats like '8:45'\n",
    "    tokens = [word for word in tokens if not re.match(r'^\\d+$', word)]  \n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Tokenized words after preprocessing\n",
    "tokens = tokenize_text(text_corpus)\n",
    "print(\"Sample Tokens:\", tokens[:20])  # Print first 20 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "44671105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram Model (Word Frequency Distribution)\n",
    "unigram_freq = FreqDist(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8828f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Model (Conditional Frequency Distribution)\n",
    "bigrams = list(nltk.bigrams(tokens))\n",
    "bigram_freq = ConditionalFreqDist(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b2cf268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram Model (Conditional Frequency Distribution)\n",
    "trigrams = list(nltk.trigrams(tokens))\n",
    "trigram_freq = ConditionalFreqDist(((w1, w2), w3) for w1, w2, w3 in trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5fd1bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract realistic sentence beginnings from the dataset\n",
    "starting_phrases = []\n",
    "for i in range(len(tokens) - 1):\n",
    "    if tokens[i] == \".\" or tokens[i] == \"!\":  # Detect sentence boundaries\n",
    "        starting_phrases.append(tokens[i+1])  # Next word is likely a sentence start\n",
    "\n",
    "# Use these starting words instead of random selection\n",
    "starting_words = list(set(starting_phrases))  # Remove duplicates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3b604896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Sentence: Raat main gya shuru b baad nashta apni ami\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### UNIGRAM GENERATION ###\n",
    "def generate_better_unigram_sentence():\n",
    "    sentence_length = random.randint(7, 12)\n",
    "    \n",
    "    # Select a first word from common starting words\n",
    "    first_word = random.choice(starting_words) if starting_words else random.choice(list(unigram_freq.keys()))\n",
    "    sentence = [first_word]\n",
    "\n",
    "    # Pick the remaining words based on unigram frequency\n",
    "    sentence += random.choices(list(unigram_freq.keys()), weights=unigram_freq.values(), k=sentence_length - 1)\n",
    "\n",
    "    return \" \".join(sentence).capitalize()\n",
    "\n",
    "print(\"Unigram Sentence:\", generate_better_unigram_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9b100b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Sentence: Shuru hogya tha ami na apna room main unhein mila aur.\n"
     ]
    }
   ],
   "source": [
    "def generate_better_bigram_sentence():\n",
    "    sentence_length = random.randint(7, 12)\n",
    "    first_word = random.choice(starting_words) if starting_words else random.choice(list(bigram_freq.keys()))\n",
    "    sentence = [first_word]\n",
    "\n",
    "    for _ in range(sentence_length - 1):\n",
    "        last_word = sentence[-1]\n",
    "        if last_word in bigram_freq:\n",
    "            next_word = random.choices(list(bigram_freq[last_word].keys()), \n",
    "                                       weights=bigram_freq[last_word].values())[0]\n",
    "            sentence.append(next_word)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return \" \".join(sentence).capitalize() + \".\"\n",
    "\n",
    "\n",
    "print(\"Bigram Sentence:\", generate_better_bigram_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ff522dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Sentence: Aj sham meri bari behn ki call ayi kuch kaam\n"
     ]
    }
   ],
   "source": [
    "### TRIGRAM GENERATION ###\n",
    "def generate_better_trigram_sentence(prev_last_word=None):\n",
    "    sentence_length = random.randint(7, 12)\n",
    "\n",
    "    # If there's a previous sentence, try to start with its last word\n",
    "    if prev_last_word and prev_last_word in bigram_freq:\n",
    "        first_two_words = (prev_last_word, random.choice(list(bigram_freq[prev_last_word].keys())))\n",
    "    else:\n",
    "        first_two_words = random.choice(list(trigram_freq.keys()))  # Pick random bigram\n",
    "\n",
    "    sentence = list(first_two_words)\n",
    "\n",
    "    for _ in range(sentence_length - 2):\n",
    "        last_two_words = tuple(sentence[-2:])\n",
    "        if last_two_words in trigram_freq:\n",
    "            next_word = random.choices(list(trigram_freq[last_two_words].keys()), \n",
    "                                       weights=trigram_freq[last_two_words].values())[0]\n",
    "            sentence.append(next_word)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return \" \".join(sentence).capitalize()\n",
    "\n",
    "\n",
    "print(\"Trigram Sentence:\", generate_better_trigram_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f057cc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (Unigram): 370.0237931408215\n",
      "Perplexity (Bigram): 185.66215786911124\n",
      "Perplexity (Trigram): 266.9431531773597\n"
     ]
    }
   ],
   "source": [
    "# Improved perplexity function\n",
    "def calculate_perplexity(model, sentence_tokens, ngram_type=\"unigram\"):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a sentence given an n-gram model.\n",
    "    \"\"\"\n",
    "    n = len(sentence_tokens)\n",
    "    prob = 1.0\n",
    "    \n",
    "    if ngram_type == \"unigram\":\n",
    "        total_count = sum(model.values())  # Total words in unigram model\n",
    "        vocab_size = len(model)\n",
    "        for word in sentence_tokens:\n",
    "            word_prob = (model.get(word, 0) + 1) / (total_count + vocab_size)  # Laplace Smoothing\n",
    "            prob *= 1 / word_prob\n",
    "    \n",
    "    elif ngram_type == \"bigram\":\n",
    "        for i in range(len(sentence_tokens) - 1):\n",
    "            w1, w2 = sentence_tokens[i], sentence_tokens[i+1]\n",
    "            if w1 in model and w2 in model[w1]:\n",
    "                word_prob = (model[w1][w2] + 1) / (sum(model[w1].values()) + len(model))\n",
    "            else:\n",
    "                word_prob = 1e-5  # Small smoothing factor for unseen bigrams\n",
    "            prob *= 1 / word_prob\n",
    "\n",
    "    elif ngram_type == \"trigram\":\n",
    "        for i in range(len(sentence_tokens) - 2):\n",
    "            w1, w2, w3 = sentence_tokens[i], sentence_tokens[i+1], sentence_tokens[i+2]\n",
    "            if (w1, w2) in model and w3 in model[(w1, w2)]:\n",
    "                word_prob = (model[(w1, w2)][w3] + 1) / (sum(model[(w1, w2)].values()) + len(model))\n",
    "            else:\n",
    "                word_prob = 1e-5\n",
    "            prob *= 1 / word_prob\n",
    "\n",
    "    return math.pow(prob, 1/n)\n",
    "\n",
    "# Example usage:\n",
    "sample_unigram_sentence = generate_better_unigram_sentence().split()\n",
    "sample_bigram_sentence = generate_better_bigram_sentence().split()\n",
    "sample_trigram_sentence = generate_better_trigram_sentence().split()\n",
    "\n",
    "print(\"Perplexity (Unigram):\", calculate_perplexity(unigram_freq, sample_unigram_sentence, \"unigram\"))\n",
    "print(\"Perplexity (Bigram):\", calculate_perplexity(bigram_freq, sample_bigram_sentence, \"bigram\"))\n",
    "print(\"Perplexity (Trigram):\", calculate_perplexity(trigram_freq, sample_trigram_sentence, \"trigram\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6d48395c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth Diary Entry:\n",
      "Jumma ka liya nikal ay tha to time guzara bazar. Bnaye aur phir sb bike nikala tyar hona. Reh gya raat ko wapis aa ga khana khaya aur phir main. Mai soo gya aj late soya tha main. Khala b jana tha wo sb bhot khush rha phir nicha.\n"
     ]
    }
   ],
   "source": [
    "def generate_smooth_diary_entry(num_sentences=5):\n",
    "    diary_entry = []\n",
    "    prev_last_word = None  \n",
    "\n",
    "    for _ in range(num_sentences):\n",
    "        # If we have a previous word, try generating a sentence based on it\n",
    "        if prev_last_word and prev_last_word in bigram_freq:\n",
    "            sentence = generate_better_trigram_sentence(prev_last_word)\n",
    "        else:\n",
    "            sentence = generate_better_bigram_sentence()\n",
    "\n",
    "        diary_entry.append(sentence)\n",
    "        prev_last_word = sentence.split()[-1]  # Update last word for the next sentence\n",
    "\n",
    "    return \" \".join(diary_entry)\n",
    "\n",
    "# Generate and print a smooth diary entry\n",
    "print(\"Smooth Diary Entry:\")\n",
    "print(generate_smooth_diary_entry())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3026dc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Bigram Sentence: Or kuch der baad main apna kamroon.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# Create reversed bigrams (right-to-left)\n",
    "reversed_bigrams = [(w2, w1) for w1, w2 in bigrams]\n",
    "backward_bigram_freq = ConditionalFreqDist(reversed_bigrams)\n",
    "\n",
    "def generate_backward_bigram_sentence():\n",
    "    sentence_length = random.randint(7, 12)\n",
    "    \n",
    "    # Select a random last word from frequent words\n",
    "    last_word = random.choice(starting_words) if starting_words else random.choice(list(unigram_freq.keys()))\n",
    "    sentence = [last_word]\n",
    "\n",
    "    for _ in range(sentence_length - 1):\n",
    "        current_word = sentence[-1]\n",
    "        if current_word in backward_bigram_freq:\n",
    "            prev_word = random.choices(list(backward_bigram_freq[current_word].keys()), \n",
    "                                       weights=backward_bigram_freq[current_word].values())[0]\n",
    "            sentence.append(prev_word)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Reverse the generated words to make the sentence readable\n",
    "    return \" \".join(sentence[::-1]).capitalize() + \".\"\n",
    "\n",
    "# Generate a backward bigram sentence\n",
    "print(\"Backward Bigram Sentence:\", generate_backward_bigram_sentence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "820c6861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional Bigram Sentence: Aya to hum na tasweerein bnayi aur phir main.\n"
     ]
    }
   ],
   "source": [
    "def generate_bidirectional_bigram_sentence():\n",
    "    sentence_length = random.randint(7, 12)\n",
    "\n",
    "    # Select a random middle word to start the sentence\n",
    "    middle_word = random.choice(starting_words) if starting_words else random.choice(list(unigram_freq.keys()))\n",
    "    sentence = [middle_word]\n",
    "\n",
    "    # Generate words forward (right)\n",
    "    for _ in range(sentence_length // 2):\n",
    "        last_word = sentence[-1]\n",
    "        if last_word in bigram_freq:\n",
    "            next_word = random.choices(list(bigram_freq[last_word].keys()), \n",
    "                                       weights=bigram_freq[last_word].values())[0]\n",
    "            sentence.append(next_word)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Generate words backward (left)\n",
    "    for _ in range(sentence_length // 2):\n",
    "        first_word = sentence[0]\n",
    "        if first_word in backward_bigram_freq:\n",
    "            prev_word = random.choices(list(backward_bigram_freq[first_word].keys()), \n",
    "                                       weights=backward_bigram_freq[first_word].values())[0]\n",
    "            sentence.insert(0, prev_word)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return \" \".join(sentence).capitalize() + \".\"\n",
    "\n",
    "# Generate a bidirectional bigram sentence\n",
    "print(\"Bidirectional Bigram Sentence:\", generate_bidirectional_bigram_sentence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab58578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
